{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae804849",
   "metadata": {},
   "source": [
    "# Databricks Production Pipeline: Real-Time Electricity Price Forecasting\n",
    "This notebook demonstrates a production-ready architecture for real-time forecasting using Databricks. It covers ETL with Structured Streaming, feature engineering, model training with MLflow, batch inference, and Delta Table storage. The workflow is designed for deployment as a Databricks Job."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3f86ce",
   "metadata": {},
   "source": [
    "## 1. Configuration & Imports\n",
    "Set up required libraries, paths, and Databricks configs. Ensure MLflow autologging is enabled for XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a5853f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import xgboost as xgb\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "# Set paths\n",
    "raw_data_path = \"/dbfs/FileStore/energy_price/raw/352_2024-06-06T0000_2025-06-06T0000.csv\"\n",
    "delta_feature_table = \"/dbfs/FileStore/energy_price/delta/features\"\n",
    "delta_prediction_table = \"/dbfs/FileStore/energy_price/delta/predictions\"\n",
    "mlflow.set_experiment(\"/Users/your_user/energy_price_xgb\")\n",
    "mlflow.xgboost.autolog()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84401bc",
   "metadata": {},
   "source": [
    "## 2. ETL & Feature Engineering with Structured Streaming\n",
    "Use Databricks Auto Loader to ingest CSV as a stream, engineer features, and write to a Delta Table. This simulates real-time ingestion for production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75533b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define schema for input data\n",
    "schema = StructType([\n",
    "    StructField(\"start_time\", StringType(), True),\n",
    "    StructField(\"end_time\", StringType(), True),\n",
    "    StructField(\"price_eur_mwh\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Read CSV as a stream (Auto Loader)\n",
    "df_stream = (\n",
    "    spark.readStream.format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(raw_data_path)\n",
    ")\n",
    "\n",
    "# Feature engineering\n",
    "feature_df = (\n",
    "    df_stream\n",
    "    .withColumn(\"start_time\", to_timestamp(col(\"start_time\")))\n",
    "    .withColumn(\"lag_1d\", lag(\"price_eur_mwh\", 1).over(Window.orderBy(\"start_time\")))\n",
    "    .withColumn(\"lag_7d\", lag(\"price_eur_mwh\", 7).over(Window.orderBy(\"start_time\")))\n",
    "    .withColumn(\"roll_mean_7d\", avg(\"price_eur_mwh\").over(Window.orderBy(\"start_time\").rowsBetween(-6, 0)))\n",
    "    .withColumn(\"roll_mean_30d\", avg(\"price_eur_mwh\").over(Window.orderBy(\"start_time\").rowsBetween(-29, 0)))\n",
    "    .withColumn(\"day_of_week\", dayofweek(\"start_time\"))\n",
    "    .withColumn(\"month\", month(\"start_time\"))\n",
    "    .withColumn(\"is_weekend\", (dayofweek(\"start_time\") >= 6).cast(\"int\"))\n",
    "    .dropna()\n",
    ")\n",
    "\n",
    "# Write features to Delta Table\n",
    "feature_query = (\n",
    "    feature_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/dbfs/FileStore/energy_price/checkpoints/features\")\n",
    "    .start(delta_feature_table)\n",
    ")\n",
    "# Wait for streaming to initialize (in production, this runs continuously)\n",
    "feature_query.awaitTermination(timeout=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4191f654",
   "metadata": {},
   "source": [
    "## 3. Model Training & Registration (XGBoost + MLflow)\n",
    "Train XGBoost on the engineered features and register the model in MLflow. In production, this can be triggered on a schedule or by new data arrival."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ccd56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features from Delta Table for training\n",
    "features_pdf = spark.read.format(\"delta\").load(delta_feature_table).toPandas()\n",
    "\n",
    "# Prepare features/target\n",
    "feature_cols = [c for c in features_pdf.columns if c not in [\"start_time\", \"end_time\", \"price_eur_mwh\"]]\n",
    "X = features_pdf[feature_cols]\n",
    "y = features_pdf[\"price_eur_mwh\"]\n",
    "\n",
    "# Train/test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\n",
    "\n",
    "# Train and log model\n",
    "with mlflow.start_run(run_name=\"xgb_train\"):\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    mlflow.xgboost.log_model(model, \"model\")\n",
    "    mlflow.log_metric(\"train_rows\", len(X_train))\n",
    "    mlflow.log_metric(\"test_rows\", len(X_test))\n",
    "    run_id = mlflow.active_run().info.run_id\n",
    "    model_uri = f\"runs:/{run_id}/model\"\n",
    "    print(f\"Model logged to: {model_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17479d28",
   "metadata": {},
   "source": [
    "## 4. Batch Inference Pipeline\n",
    "Load new feature data, run batch inference with the latest XGBoost model, and write predictions to a Delta Table. This can be triggered by new data or on a schedule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1c8734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load latest model from MLflow\n",
    "latest_model = mlflow.xgboost.load_model(model_uri)\n",
    "\n",
    "# Load new features for prediction (simulate with test set)\n",
    "X_pred = X_test.copy()\n",
    "pred_df = features_pdf.iloc[X_test.index][[\"start_time\"]].copy()\n",
    "pred_df[\"prediction\"] = latest_model.predict(X_pred)\n",
    "\n",
    "# Convert to Spark DataFrame and write to Delta Table\n",
    "pred_sdf = spark.createDataFrame(pred_df)\n",
    "pred_sdf.write.format(\"delta\").mode(\"append\").save(delta_prediction_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a27add",
   "metadata": {},
   "source": [
    "## 5. Databricks SQL Dashboard\n",
    "Create a Databricks SQL Dashboard to visualize predictions from the Delta Table. (This step is done in the Databricks UI: create a new dashboard, add a query on the `delta_prediction_table`, and set auto-refresh.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a18d2e",
   "metadata": {},
   "source": [
    "## 6. Databricks Workflow (Job) Setup\n",
    "To automate this pipeline, create a Databricks Job with the following tasks:\n",
    "- Run this notebook for ETL & feature engineering\n",
    "- Run this notebook for model training\n",
    "- Run this notebook for batch inference\n",
    "\n",
    "Example JSON for Databricks Job (replace paths as needed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cabcd51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Databricks Job JSON (for documentation)\n",
    "import json\n",
    "job_json = {\n",
    "    \"name\": \"Energy Price Forecasting Pipeline\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"etl_feature_engineering\",\n",
    "            \"notebook_task\": {\"notebook_path\": \"/Repos/your_user/energy_price/project/databricks_production_pipeline.ipynb\", \"base_parameters\": {\"step\": \"etl\"}},\n",
    "            \"cluster_spec\": {\"existing_cluster_id\": \"YOUR_CLUSTER_ID\"}\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"model_training\",\n",
    "            \"depends_on\": [{\"task_key\": \"etl_feature_engineering\"}],\n",
    "            \"notebook_task\": {\"notebook_path\": \"/Repos/your_user/energy_price/project/databricks_production_pipeline.ipynb\", \"base_parameters\": {\"step\": \"train\"}},\n",
    "            \"cluster_spec\": {\"existing_cluster_id\": \"YOUR_CLUSTER_ID\"}\n",
    "        },\n",
    "        {\n",
    "            \"task_key\": \"batch_inference\",\n",
    "            \"depends_on\": [{\"task_key\": \"model_training\"}],\n",
    "            \"notebook_task\": {\"notebook_path\": \"/Repos/your_user/energy_price/project/databricks_production_pipeline.ipynb\", \"base_parameters\": {\"step\": \"inference\"}},\n",
    "            \"cluster_spec\": {\"existing_cluster_id\": \"YOUR_CLUSTER_ID\"}\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "print(json.dumps(job_json, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e79f66",
   "metadata": {},
   "source": [
    "---\n",
    "**Summary:**\n",
    "- This notebook is ready for Databricks deployment and can be orchestrated as a Job.\n",
    "- All data is stored in Delta Tables for reliability and performance.\n",
    "- Model is tracked and versioned with MLflow.\n",
    "- Dashboarding is handled in Databricks SQL.\n",
    "- Adjust paths and cluster IDs as needed for your workspace."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
