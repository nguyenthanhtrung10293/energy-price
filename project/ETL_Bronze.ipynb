{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4af890d1",
   "metadata": {},
   "source": [
    "# Bronze Table Ingestion (Raw Data)\n",
    "Ingest raw CSV data into a Delta bronze table using Databricks Auto Loader. This is the first step in the medallion architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b89368",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest raw CSV to bronze Delta table using Databricks Auto Loader\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define the path for the bronze Delta table and raw data\n",
    "bronze_path = \"/Volumes/your_catalog/your_schema/energy_price_bronze\"\n",
    "raw_data_path = \"/dbfs/FileStore/energy_price/raw/352_2024-06-06T0000_2025-06-06T0000.csv\"\n",
    "\n",
    "# Define schema for the raw data\n",
    "schema = StructType([\n",
    "    StructField(\"start_time\", StringType(), True),\n",
    "    StructField(\"end_time\", StringType(), True),\n",
    "    StructField(\"price_eur_mwh\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Use Auto Loader for scalable ingestion (can be adapted for cloud storage)\n",
    "bronze_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(raw_data_path)\n",
    ")\n",
    "\n",
    "# Write to Delta bronze table (overwrite for demo, use append in production)\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)\n",
    "print(f\"Bronze table written to {bronze_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ab6ce1",
   "metadata": {},
   "source": [
    "# Bronze Table Ingestion (Raw Data)\n",
    "Ingest raw CSV data into a Delta bronze table using Databricks Auto Loader. This is the first step in the medallion architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54a524d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest raw CSV to bronze Delta table using Databricks Auto Loader\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Define the path for the bronze Delta table and raw data\n",
    "bronze_path = \"/Volumes/your_catalog/your_schema/energy_price_bronze\"\n",
    "raw_data_path = \"/dbfs/FileStore/energy_price/raw/352_2024-06-06T0000_2025-06-06T0000.csv\"\n",
    "\n",
    "# Define schema for the raw data\n",
    "schema = StructType([\n",
    "    StructField(\"start_time\", StringType(), True),\n",
    "    StructField(\"end_time\", StringType(), True),\n",
    "    StructField(\"price_eur_mwh\", DoubleType(), True)\n",
    "])\n",
    "\n",
    "# Use Auto Loader for scalable ingestion (can be adapted for cloud storage)\n",
    "bronze_df = (\n",
    "    spark.read.format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema)\n",
    "    .load(raw_data_path)\n",
    ")\n",
    "\n",
    "# Write to Delta bronze table (overwrite for demo, use append in production)\n",
    "bronze_df.write.format(\"delta\").mode(\"overwrite\").save(bronze_path)\n",
    "print(f\"Bronze table written to {bronze_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
